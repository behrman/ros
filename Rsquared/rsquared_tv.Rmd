---
title: "Regression and Other Stories: Bayesian $R^2$"
author: "Andrew Gelman, Jennifer Hill, Aki Vehtari"
date: "`r Sys.Date()`"
output:
  github_document:
    toc: true
---
Tidyverse version by Bill Behrman.

Bayesian $R^2$. See Chapter 11 in Regression and Other Stories.

See also
- Andrew Gelman, Ben Goodrich, Jonah Gabry, and Aki Vehtari (2018).
  R-squared for Bayesian regression models. The American Statistician, 73:307-209
  [doi:10.1080/00031305.2018.1549100](https://doi.org/10.1080/00031305.2018.1549100).

-------------

```{r, message=FALSE}
# Packages
library(tidyverse)
library(rstanarm)

# Parameters
  # Seed
SEED <- 1800
  # Kid test score data
file_kids <- here::here("KidIQ/data/kidiq.csv")
  # Common code
file_common <- here::here("_common.R")

#===============================================================================

# Run common code
source(file_common)
```

# 11 Assumptions, diagnostics, and model evaluation

## 11.6 Residual standard deviation Ïƒ and explained variance $R^2$

### Bayesian $R^2$

#### Small dataset

Data

```{r}
data <- tibble(x = -2:2, y = c(-1.3, -0.4, -0.5, 1.4, 0.8))

data
```

Least-squares fit.

```{r}
fit_lm <- lm(y ~ x, data = data)

broom::tidy(fit_lm)

broom::glance(fit_lm)$r.squared
```

The least-square fit has an intercept of essentially 0 and a slope of 0.6. It's $R^2$ is `r broom::glance(fit_lm)$r.squared`.

Bayes fit with a strong prior. The intercept prior has a mean of 0, and the slope prior has a mean of 1.

```{r}
fit_bayes <- 
  stan_glm(
    y ~ x,
    data = data,
    refresh = 0,
    seed = SEED,
    prior = normal(location = 1, scale = 0.2, autoscale = FALSE),
    prior_intercept = normal(location = 0, scale = 0.2, autoscale = FALSE),
    prior_aux = NULL
  )

fit_bayes
```

The Bayes fit has an intercept of `r coef(fit_bayes)[["(Intercept)"]]` and a slope of `r coef(fit_bayes)[["x"]]`. The slope is between the least-square slope and the prior slope.

Bayesian $R^2$.

```{r}
bayes_r2 <- bayes_R2(fit_bayes)
median(bayes_r2)
```

Least squares and Bayes fits.

```{r, fig.asp=0.75}
intercept_lm <- coef(fit_lm)[["(Intercept)"]]
slope_lm <- coef(fit_lm)[["x"]]
intercept_bayes <- coef(fit_bayes)[["(Intercept)"]]
slope_bayes <- coef(fit_bayes)[["x"]]

lines <-
  tribble(
    ~intercept, ~slope, ~label,
    intercept_lm, slope_lm, "Least squares",
    0, 1, "Bayes prior", 
    intercept_bayes, slope_bayes, "Bayes posterior",
  )

data %>%
  ggplot(aes(x, y)) +
  geom_abline(
    aes(slope = slope, intercept = intercept, color = label, linetype = label),
    data = lines
  ) +
  geom_point() +
  coord_fixed(ylim = c(-2, 2)) +
  scale_color_manual(
    breaks = c("Least squares", "Bayes prior", "Bayes posterior"),
    values = c("black", "red", "red")
  ) +
  scale_linetype_manual(
    breaks = c("Least squares", "Bayes prior", "Bayes posterior"),
    values = c("solid", "dashed", "solid")
  ) +
  labs(
    title = "Least squares and Bayes fits",
    color = NULL,
    linetype = NULL
  )
```

Bayes posterior simulations. 

```{r, fig.asp=0.75}
set.seed(374)

n_lines <- 20

data %>%
  ggplot(aes(x, y)) +
  geom_abline(
    aes(slope = x, intercept = `(Intercept)`),
    data = as_tibble(fit_bayes) %>% slice_sample(n = n_lines),
    alpha = 0.25
  ) +
  geom_abline(slope = slope_bayes, intercept = intercept_bayes, color = "red") +
  geom_point() +
  coord_fixed(ylim = c(-2, 2)) +
  labs(title = "Bayes posterior simulations")
```

Posterior distribution of Bayesian $R^2$.

```{r, fig.asp=0.75}
tibble(bayes_r2) %>% 
  ggplot(aes(bayes_r2)) +
  geom_histogram(binwidth = 0.05, boundary = 0) +
  geom_vline(xintercept = median(bayes_r2), color = "red") +
  labs(
    title = expression(paste("Posterior distribution of Bayesian ", R^2)),
    subtitle = "Vertical line is the median",
    x = expression(paste("Bayesian ", R^2)),
    y = "Count"
  )
```

#### Child test scores

Data

```{r, message=FALSE}
kids <- read_csv(file_kids)

kids
```

Bayes fit of child test score vs. mother high school completion and IQ.

```{r}
set.seed(765)

fit <- stan_glm(kid_score ~ mom_hs + mom_iq, data = kids, refresh = 0)

fit
```

Bayesian $R^2$.

```{r}
bayes_r2 <- bayes_R2(fit)
median(bayes_r2)
```

Posterior distribution of Bayesian $R^2$.

```{r, fig.asp=0.75}
tibble(bayes_r2) %>% 
  ggplot(aes(bayes_r2)) +
  geom_histogram(binwidth = 0.01, boundary = 0) +
  geom_vline(xintercept = median(bayes_r2), color = "red") +
  labs(
    title = expression(paste("Posterior distribution of Bayesian ", R^2)),
    subtitle = "Vertical line is the median",
    x = expression(paste("Bayesian ", R^2)),
    y = "Count"
  )
```

