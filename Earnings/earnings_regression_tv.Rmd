---
title: "Regression and Other Stories: Earnings"
author: "Andrew Gelman, Jennifer Hill, Aki Vehtari"
date: "`r Sys.Date()`"
output:
  github_document:
    toc: true
---
Tidyverse version by Bill Behrman.

Predict respondents' yearly earnings using survey data from
1990. See Chapters 6 and 12 in Regression and Other Stories.

-------------

```{r, message=FALSE}
# Packages
library(tidyverse)
library(bayesplot)
library(rstanarm)

# Parameters
  # Seed
SEED <- 7783
  # Earnings data
file_earnings <- here::here("Earnings/data/earnings.csv")
  # Common code
file_common <- here::here("_common.R")

# Functions
  # Plot kernel density of data and sample replicates
plot_density_overlay <- function(y, y_rep) {
  ggplot(mapping = aes(y)) +
    stat_density(
      aes(group = rep, color = "y_rep"),
      data = 
        seq_len(nrow(y_rep)) %>% map_dfr(~ tibble(rep = ., y = y_rep[., ])),
      geom = "line",
      position = "identity",
      alpha = 0.5,
      size = 0.25
    ) +
    stat_density(aes(color = "y"), data = tibble(y), geom = "line", size = 1) +
    scale_y_continuous(breaks = 0) +
    scale_color_discrete(
      breaks = c("y", "y_rep"),
      labels = c("y", expression(y[rep]))
    ) +
    theme(legend.text.align = 0) +
    labs(
      x = NULL,
      y = NULL,
      color = NULL
    )
}

#===============================================================================

# Run common code
source(file_common)
```

# 6 Background on regression modeling

## 6.3 Interpret coefficients as comparisons, not effects

Data

```{r, message=FALSE}
earnings <- 
  file_earnings %>% 
  read_csv() %>% 
  mutate(
    sex = 
      case_when(
        male == 0 ~ "Female",
        male == 1 ~ "Male",
        TRUE ~ NA_character_
      )
  )

earnings %>% 
  select(height, sex, earn)
```

Fit linear regression of earnings on height and sex with no interaction.

```{r}
fit_2 <- 
  stan_glm(earn ~ height + sex, data = earnings, refresh = 0, seed = SEED)

print(fit_2)
```

Linear regression of earnings on height and sex with no interaction.

```{r}
lines <- 
  tribble(
    ~sex, ~intercept, ~slope,
    "Female", coef(fit_2)[["(Intercept)"]], coef(fit_2)[["height"]],
    "Male", 
      coef(fit_2)[["(Intercept)"]] + coef(fit_2)[["sexMale"]],
      coef(fit_2)[["height"]]
  )

offset <- 0.2

earnings %>% 
  mutate(
    height =
      case_when(
        sex == "Female" ~ height - offset,
        sex == "Male" ~ height + offset,
        TRUE ~ NA_real_
      )
  ) %>% 
  ggplot(aes(height, earn, color = sex)) +
  geom_count() +
  geom_abline(
    aes(slope = slope, intercept = intercept, color = sex),
    data = lines
  ) +
  coord_cartesian(ylim = c(0, 1e5)) +
  scale_x_continuous(breaks = scales::breaks_width(1), minor_breaks = NULL) +
  scale_y_continuous(labels = scales::label_comma()) +
  theme(legend.position = "bottom") +
  labs(
    title = 
      "Linear regression of earnings on height and sex with no interaction",
    x = "Height",
    y = "Earnings",
    color = "Sex",
    size = "Count"
  )
```

The equations for the regression lines are:

    Men:   y = `r format(lines$intercept[lines$sex == "Male"], digits = 0, nsmall = 0, scientific = FALSE)` + `r format(lines$slope[lines$sex == "Male"], digits = 0, nsmall = 0, scientific = FALSE)` x
    Women: y = `r format(lines$intercept[lines$sex == "Female"], digits = 0, nsmall = 0, scientific = FALSE)` + `r format(lines$slope[lines$sex == "Female"], digits = 0, nsmall = 0, scientific = FALSE)` x

Fit linear regression of earnings on height and sex with interaction.

```{r}
fit_3 <- 
  stan_glm(
    earn ~ height + sex + height:sex,
    data = earnings,
    refresh = 0,
    seed = SEED
  )

print(fit_3)
```

Linear regression of earnings on height and sex with interaction.

```{r}
lines <- 
  tribble(
    ~sex, ~intercept, ~slope,
    "Female", coef(fit_3)[["(Intercept)"]], coef(fit_3)[["height"]],
    "Male", 
      coef(fit_3)[["(Intercept)"]] + coef(fit_3)[["sexMale"]],
      coef(fit_3)[["height"]] + coef(fit_3)[["height:sexMale"]]
  )

offset <- 0.2

earnings %>% 
  mutate(
    height =
      case_when(
        sex == "Female" ~ height - offset,
        sex == "Male" ~ height + offset,
        TRUE ~ NA_real_
      )
  ) %>% 
  ggplot(aes(height, earn, color = sex)) +
  geom_count() +
  geom_abline(
    aes(slope = slope, intercept = intercept, color = sex), 
    data = lines
  ) +
  coord_cartesian(ylim = c(0, 1e5)) +
  scale_x_continuous(breaks = scales::breaks_width(1), minor_breaks = NULL) +
  scale_y_continuous(labels = scales::label_comma()) +
  theme(legend.position = "bottom") +
  labs(
    title = 
      "Linear regression of earnings on height and sex with interaction",
    x = "Height",
    y = "Earnings",
    color = "Sex",
    size = "Count"
  )
```

The equations for the regression lines are:

    Men:   y = `r format(lines$intercept[lines$sex == "Male"], digits = 0, nsmall = 0, scientific = FALSE)` + `r format(lines$slope[lines$sex == "Male"], digits = 0, nsmall = 0, scientific = FALSE)` x
    Women: y =  `r format(lines$intercept[lines$sex == "Female"], digits = 0, nsmall = 0, scientific = FALSE)` + `r format(lines$slope[lines$sex == "Female"], digits = 0, nsmall = 0, scientific = FALSE)` x

From the plots, we can see that many more women than men have no earnings.

```{r}
earnings %>% 
  count(earn, sex) %>% 
  group_by(sex) %>% 
  mutate(prop = n / sum(n)) %>% 
  ungroup() %>% 
  filter(earn == 0)
```

15% of women have no earnings, whereas only 2% of men have no earnings.

# 12 Transformations and regression

## 12.4 Logarithmic transformations

### Earnings and height example

#### Direct interpretation of small coefficients on the log scale

Linear regression with log earnings as outcome.

```{r}
fit_log_1 <- 
  stan_glm(
    log(earn) ~ height,
    data = earnings %>% filter(earn > 0),
    seed = SEED,
    refresh = 0
  )

print(fit_log_1, digits = 2)
```

Earnings vs. height on log scale: With 50% and 90% predictive intervals.

```{r, fig.asp=0.75}
v <- 
  tibble(height = seq_range(earnings$height)) %>% 
  predictive_intervals(fit = fit_log_1) %>% 
  mutate(across(!height, exp))

v %>% 
  ggplot(aes(height)) +
  geom_ribbon(aes(ymin = `5%`, ymax = `95%`), alpha = 0.25) +
  geom_ribbon(aes(ymin = `25%`, ymax = `75%`), alpha = 0.5) +
  geom_line(aes(y = .pred)) +
  geom_count(aes(y = earn), data = earnings %>% filter(earn > 0)) +
  scale_y_log10(labels = scales::label_comma()) +
  theme(legend.position = "bottom") +
  labs(
    title = "Earnings vs. height on log scale",
    subtitle = "With 50% and 90% predictive intervals",
    x = "Height",
    y = "Earnings",
    size = "Count"
  )
```

Earnings vs. height: With 50% and 90% predictive intervals.

```{r, fig.asp=0.75}
v %>% 
  ggplot(aes(height)) +
  geom_ribbon(aes(ymin = `5%`, ymax = `95%`), alpha = 0.25) +
  geom_ribbon(aes(ymin = `25%`, ymax = `75%`), alpha = 0.5) +
  geom_line(aes(y = .pred)) +
  geom_count(aes(y = earn), data = earnings %>% filter(earn > 0)) +
  coord_cartesian(ylim = c(0, 1e5)) +
  scale_y_continuous(labels = scales::label_comma()) +
  theme(legend.position = "bottom") +
  labs(
    title = "Earnings vs. height",
    subtitle = "With 50% and 90% predictive intervals",
    x = "Height",
    y = "Earnings",
    size = "Count"
  )
```

#### Predictive checking

Linear regression with non-log, positive earnings.

```{r}
fit_1 <- 
  stan_glm(
    earn ~ height,
    data = earnings %>% filter(earn > 0),
    seed = SEED,
    refresh = 0
  )

print(fit_1, digits = 2)
```

Simulate new data for non-log model.

```{r}
set.seed(377)

y_rep_1 <- posterior_predict(fit_1)

n_sims <- nrow(y_rep_1)
n_rep <- 100
sims_sample <- sample(n_sims, n_rep)
```

Kernel density of data and `r n_rep` sample replicates from non-log model.

```{r}
plot_density_overlay(
  y = earnings$earn %>% keep(. > 0),
  y_rep = y_rep_1[sims_sample, ]
) +
  labs(
    title = 
      str_glue(
        "Kernel density of data and {n_rep} sample replicates from non-log model"
      ),
    x = "Earnings"
  )
```

Kernel density of data and `r n_rep` sample replicates from non-log model using bayesplot.

```{r}
ppc_dens_overlay(
  y = earnings$earn %>% keep(. > 0),
  yrep = y_rep_1[sims_sample, ]
) +
  theme(
    axis.line.y = element_blank(),
    text = element_text(family = "sans")
  ) +
  labs(title = "earn")
```

Simulate new data for log model.

```{r}
set.seed(377)

y_rep_log_1 <- posterior_predict(fit_log_1)
```

Kernel density of data and `r n_rep` sample replicates from log model.

```{r}
plot_density_overlay(
  y = earnings$earn %>% keep(. > 0) %>% log(),
  y_rep = y_rep_log_1[sims_sample, ]
) +
  scale_x_continuous(breaks = scales::breaks_width(2)) +
  labs(
    title = 
      str_glue(
        "Kernel density of data and {n_rep} sample replicates from log model"
      ),
    x = "Log earnings"
  )
```

Kernel density of data and `r n_rep` sample replicates from log model using bayesplot.

```{r}
ppc_dens_overlay(
  y = earnings$earn %>% keep(. > 0) %>% log(),
  yrep = y_rep_log_1[sims_sample, ]
) +
  theme(
    axis.line.y = element_blank(),
    text = element_text(family = "sans")
  ) +
  labs(title = "log(earn)")
```

### Why we use natural log rather than log base 10

Linear regression with log10 earnings as outcome.

```{r}
fit_log10_1 <- 
  stan_glm(
    log10(earn) ~ height,
    data = earnings %>% filter(earn > 0),
    seed = SEED,
    refresh = 0
  )

print(fit_log10_1, digits = 2)
```

The `height` coefficient of 0.0247 tells us that a difference of 1 inch in height corresponds to a difference of 0.0247 in $\log_{10}$(earnings), that is a multiplicative difference of $10^{0.0247}$ = 1.06. That is the same as the 6% change as before, but it cannot be seen by simply looking at the coefficient as could be done in the natural-log case.

### Building a regression model on the log scale

#### Adding another predictor

Linear regression of log earnings with both height and sex as predictors.

```{r}
fit_log_2 <- 
  stan_glm(
    log(earn) ~ height + sex,
    data = earnings %>% filter(earn > 0),
    seed = SEED,
    refresh = 0
  )

print(fit_log_2, digits = 2)
```

#### Including an interaction

We now consider a model with an interaction between height and sex, so that the predictive comparison for height can differ for men and women.

```{r}
fit_log_3 <- 
  stan_glm(
    log(earn) ~ height + sex + height:sex,
    data = earnings %>% filter(earn > 0),
    seed = SEED,
    refresh = 0
  )

print(fit_log_3, digits = 2)
```

#### Linear transformation to make coefficients more interpretable

Create rescaled variable `height_z` to have mean 0 and standard deviation 1.

```{r}
earnings <- 
  earnings %>% 
  mutate(height_z = (height - mean(height)) / sd(height))
```

The previous linear regression using rescaled height.

```{r}
fit_log_4 <- 
  stan_glm(
    log(earn) ~ height_z + sex + height_z:sex,
    data = earnings %>% filter(earn > 0),
    seed = SEED,
    refresh = 0
  )

print(fit_log_4, digits = 2)
```

### Log-log model: transforming the input and outcome variables

If the log transformation is applied to an input variable as well as the outcome, the coefficient can be interpreted as the proportional difference in y per proportional difference in x. For example:

```{r}
fit_log_5 <- 
  stan_glm(
    log(earn) ~ log(height) + sex,
    data = earnings %>% filter(earn > 0),
    seed = SEED,
    refresh = 0
  )

print(fit_log_5, digits = 2)
```

For each 1% difference in height, the predicted difference in earnings is `r format(coef(fit_log_5)[["log(height)"]], digits = 2, nsmall = 2)`%

